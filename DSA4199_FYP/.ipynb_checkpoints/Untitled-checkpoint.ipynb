{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9064e173",
   "metadata": {},
   "source": [
    "## Comparitive Analysis of Classification Metrics\n",
    "1. Comparative Analysis between binary and multi-class datasets with:\n",
    "    - Accuracy\n",
    "    - Precision and Recall\n",
    "    - F1 Score\n",
    "    - MCC\n",
    "To compare MCC with other metrics score on both binary and multi-class datasets, the method calculate_classification_metrics() sumarises the values of all metrics (i.e. accuracy, precision, recall, F1, and MCC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a530e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_metrics(y_true, y_pred):\n",
    "    # Calculate each metric\n",
    "    accuracy = sm.accuracy_score(y_true, y_pred)\n",
    "    recall = sm.recall_score(y_true, y_pred, average='binary')\n",
    "    precision = sm.precision_score(y_true, y_pred, average='binary')\n",
    "    f1 = sm.f1_score(y_true, y_pred, average='binary')\n",
    "    mcc = sm.matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
    "    \n",
    "    return accuracy, recall, precision, f1, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a760a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "n_ground_truths = 5000\n",
    "\n",
    "def generate_data(p):\n",
    "    rng = np.random.default_rng(42)\n",
    "    # generate predictions, assumed to be perfectly calibrated\n",
    "    y_pred = rng.random(n_samples)**p # p adjusts class balance\n",
    "    # generate possible ground truths\n",
    "    y_true = (rng.random((n_samples, n_ground_truths)) \n",
    "              < y_pred[:, np.newaxis]).astype(int)\n",
    "    for j in range(n_ground_truths):\n",
    "        assert len(np.unique(y_true[:,j]))==2\n",
    "    return y_pred, y_true\n",
    "\n",
    "def neg_mcc(y_pred, y_true, t):\n",
    "    mcc = []\n",
    "    for j in range(n_ground_truths):\n",
    "        mcc.append(np.corrcoef(y_true[:,j], (y_pred>t).astype(np.int64))[0,1])\n",
    "    return -np.mean(mcc)\n",
    "\n",
    "def neg_acc(y_pred, y_true, t):\n",
    "    return -np.mean(y_true==(y_pred>t).astype(np.int64)[:, np.newaxis])\n",
    "\n",
    "p_list = [0.25, 0.5, 1.0, 1.5, 2.0]\n",
    "positive_weights = []\n",
    "thresholds_mcc = []\n",
    "thresholds_acc = []\n",
    "for p in tqdm(p_list):\n",
    "    y_pred, y_true = generate_data(p)\n",
    "    positive_weights.append(y_pred.mean())\n",
    "    res = minimize(lambda t: neg_mcc(y_pred, y_true, t), \n",
    "                   [0.5], method='Nelder-Mead', bounds=[(0,1)])\n",
    "    thresholds_mcc.append(res.x[0])\n",
    "    res = minimize(lambda t: neg_acc(y_pred, y_true, t), \n",
    "                   [0.5], method='Nelder-Mead', bounds=[(0,1)])\n",
    "    thresholds_acc.append(res.x[0])\n",
    "plt.plot(positive_weights, thresholds_mcc, '-o', label='MCC')\n",
    "plt.plot(positive_weights, thresholds_acc, '-o', label='Accuracy')\n",
    "plt.xlabel('positive weight')\n",
    "plt.ylabel('optimal threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
